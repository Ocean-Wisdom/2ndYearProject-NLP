{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f027718",
   "metadata": {},
   "source": [
    "### Training LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf4a2c",
   "metadata": {},
   "source": [
    "The following code is designed to train 32 different models by combination of different pre-processing methods. The order of the below processes reflects the order of the preprocessing methods.\n",
    "<list>\n",
    "- Expansion of contractions\n",
    "- Basic preprocessing\n",
    "- Grammar correction\n",
    "- Simplification of negations\n",
    "- Lemmatization\n",
    "- Removal of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8db1ee",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a06b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MUSIC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MUSIC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import imports as ii\n",
    "import functions as f\n",
    "import preprocessing as pp\n",
    "import neuralnetworks as nn\n",
    "import trainRNN as trainRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd07f8",
   "metadata": {},
   "source": [
    "#### Read-in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a4d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  100000\n",
      "Number of data:  10000\n"
     ]
    }
   ],
   "source": [
    "PATH = {}\n",
    "PATH[\"dataset_classification\"] = \"dataset/classification/\"\n",
    "PATH[\"dataset_labeling\"] = \"dataset/seq_labeling/\"\n",
    "PATH[\"music_reviews_train\"] = PATH[\"dataset_classification\"] + \"music_reviews_train.json.gz\"\n",
    "PATH[\"music_reviews_dev\"] = PATH[\"dataset_classification\"] + \"music_reviews_dev.json.gz\"\n",
    "PATH[\"music_reviews_test\"] = PATH[\"dataset_classification\"] + \"music_reviews_test.json.gz\"\n",
    "train = f.readJson(PATH[\"music_reviews_train\"])\n",
    "test = f.readJson(PATH[\"music_reviews_dev\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951860c3",
   "metadata": {},
   "source": [
    "#### Define features of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88a9c2",
   "metadata": {},
   "source": [
    "Below variables defining:\n",
    "<list>\n",
    "- leng = how many sentence will be used for the training\n",
    "- maxlen = maximum length of words in each sentences both in the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe92bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "leng = 30000\n",
    "maxlen = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03fcbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train and test data\n",
    "\n",
    "X_train, y_train, train_idx, train_missing_idx = f.json_divide(train)\n",
    "X_test, y_test, test_idx, test_missing_idx = f.json_divide(test)\n",
    "\n",
    "# convert labels\n",
    "\n",
    "sent_dict = {\"positive\": 1, \"negative\": 0}\n",
    "y_train = pp.sentiment_converter(y_train, sent_dict)\n",
    "y_test = pp.sentiment_converter(y_test, sent_dict)\n",
    "\n",
    "# sampling\n",
    "\n",
    "start = 6666 # just a number\n",
    "\n",
    "X_train = X_train[start:start+leng]\n",
    "y_train = y_train[start:start+leng]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d8f5b",
   "metadata": {},
   "source": [
    "#### Grid search & Training\n",
    "Below code generating 32 combinations of each preprocessing methods on the train and test set. The structure of the generated datasets is:\n",
    "<list> \n",
    "- data_sets[0] - First preprocessing combination\n",
    "- data_set[0][0] - Labels array of the combinations. For instance:  [0,1,0,0,0,0] meaning only the basic pre-processing was applied, the rest was not\n",
    "- data_sets[0][1] - Preprocessed Train set\n",
    "- datasets[0][2] - Preprocessed Dev set\n",
    "    \n",
    "</list>\n",
    "\n",
    "It's running for a while since it is generating a relatively huge set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a72e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets, y_train, y_test = f.grid_search(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693aab6",
   "metadata": {},
   "source": [
    "#### Ratio of fully catched sentences\n",
    "Basic statistics about the ratio of sentencies which were fully catched with the given maximum sentence length variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.statistics_sets_sizes(data_sets, max_len=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54768ae",
   "metadata": {},
   "source": [
    "### Training the LSTM model\n",
    "Training and evaulation takes around one hour with 5000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2846878",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainRNN.trainRNN(data_sets, y_train, y_test, early_stop_patience=2, filename=\"dummy\", maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda9969",
   "metadata": {},
   "source": [
    "### Re-training an LSTM model\n",
    "The code below is written for train a model with a given pre-process combination or combinations. Note: Early stopping criteria set to 10, meaning model is able to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4124394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinations:  [0, 1, 0, 0, 1, 0]\n",
      "Epoch 1/10\n",
      " 681/1200 [================>.............] - ETA: 42s - loss: 8.9551 - accuracy: 0.4194"
     ]
    }
   ],
   "source": [
    "# define combinations\n",
    "combination = [[0, 1, 0, 0, 1, 0],\n",
    "[0, 1, 1, 0, 1, 0],\n",
    "[0, 1, 1, 1, 1, 0],\n",
    "[0, 1, 0, 1, 1, 0],\n",
    "[1, 1, 1, 1, 1, 0],\n",
    "[1, 1, 0, 0, 1, 0],\n",
    "[1, 1, 1, 0, 1, 0],\n",
    "[1, 1, 0, 1, 1, 0],\n",
    "[0, 1, 0, 1, 1, 1],\n",
    "[0, 1, 0, 0, 1, 1],\n",
    "[1, 1, 0, 0, 1, 1],\n",
    "[1, 1, 0, 1, 1, 1],\n",
    "[0, 1, 1, 1, 1, 1],\n",
    "[0, 1, 1, 0, 1, 1],\n",
    "[1, 1, 1, 0, 1, 1],\n",
    "[1, 1, 1, 1, 1, 1],\n",
    "]\n",
    "\n",
    "# name of the file which will contain the LOGS\n",
    "filename = \"30k/retraining\"\n",
    "\n",
    "# early stopping criteria - max 10! \n",
    "early_stop_patience = 2\n",
    "\n",
    "for comb in combination:\n",
    "    data_sets, y_train, y_test = f.grid_search_retrain(X_train, X_test, y_train, y_test, comb)\n",
    "    trainRNN.trainRNN(data_sets, y_train, y_test, early_stop_patience=early_stop_patience, filename=filename, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002d208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
