{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "881d1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "import functions as f\n",
    "import preprocessing as pp\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "858e3a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  100000\n",
      "Number of data:  10000\n"
     ]
    }
   ],
   "source": [
    "PATH = {}\n",
    "PATH[\"dataset_classification\"] = \"dataset/classification/\"\n",
    "PATH[\"dataset_labeling\"] = \"dataset/seq_labeling/\"\n",
    "PATH[\"music_reviews_train\"] = PATH[\"dataset_classification\"] + \"music_reviews_train.json.gz\"\n",
    "PATH[\"music_reviews_dev\"] = PATH[\"dataset_classification\"] + \"music_reviews_dev.json.gz\"\n",
    "PATH[\"music_reviews_test\"] = PATH[\"dataset_classification\"] + \"music_reviews_test_masked.json.gz\"\n",
    "train = f.readJson(PATH[\"music_reviews_train\"])\n",
    "test = f.readJson(PATH[\"music_reviews_dev\"])\n",
    "X_test, y_test, test_idx, test_missing_idx = f.json_divide(test)\n",
    "X_train, y_train, train_idx, train_missing_idx = f.json_divide(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b4293e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aa335646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences before basic-preprocessing\n",
      "***0***\n",
      "--Before\n",
      "My dentist recommended this as a relaxation technique for dental visits. They give me an ipod with headphones, play this on it and it relieves some of the stress of dental treatment, which I dislike intensely.\n",
      "It worked so well that I bought my own copy to try at home. I fall asleep after a couple of minutes and stay asleep. Instead of tossing and turning, I hardly move at all. Highly recommend.\n",
      "--After\n",
      "My dentist recommended this as a relaxation technique for dental visits . They give me an ipod with headphones play this on it and it relieves some of stress of dental treatment which I dislike intensely . It worked so well that I bought my own copy to try at home . I fall asleep after a couple of minutes and stay asleep . Instead of tossing and turning I hardly move at all . Highly recommend .\n"
     ]
    }
   ],
   "source": [
    "def compare_b4_after_basicpreprocessing(sentences):\n",
    "    # compare b4 and after basic-preprocessing\n",
    "    print(\"Sentences before basic-preprocessing\")\n",
    "    for i, s in enumerate(sentences): \n",
    "        print(f\"***{i}***\")\n",
    "        print(\"--Before\")\n",
    "        print(s)\n",
    "        print(\"--After\")\n",
    "        tokens = []\n",
    "        for token in pp.basic_preprocess(s.split()):\n",
    "            tokens.extend([t for t in token])\n",
    "        s_preprocessed = \" \".join(tokens)\n",
    "        print(s_preprocessed)\n",
    "        \n",
    "#         print(\"---- DIFFERENCE\")\n",
    "        \n",
    "#         d = difflib.Differ()\n",
    "#         diff = d.compare([s], [s_preprocessed])\n",
    "#         print('\\n'.join(diff))\n",
    "\n",
    "#         output_list = [li for li in difflib.ndiff(s, s_preprocessed) if li[0] != ' ']\n",
    "#         print(output_list)\n",
    "compare_b4_after_basicpreprocessing(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bert_and_keras_tokenizer(sentences):\n",
    "    # compare berttokenizer and keras tokenizer \n",
    "    start = 6666\n",
    "    no = 5000\n",
    "    kt = pp.tokenizer_init(X_train[start:(start+no)], X_test) # what should it be trained on? Fuck\n",
    "    bt = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    for i, s in enumerate(sentences):\n",
    "        print(f\"***{i}***\")\n",
    "        print(\"--Before\")\n",
    "        print(s)\n",
    "        print(\"--After bert tokenization\")\n",
    "        print(\" \".join(bt.tokenize(s)))\n",
    "        print(\"--After keras tokenization\")\n",
    "        sequence = kt.texts_to_sequences(s)\n",
    "        tokens = kt.sequences_to_texts(sequence)\n",
    "        sentence = \"\"\n",
    "        for t in tokens:\n",
    "            if t != \"\": sentence += t\n",
    "            else: sentence += \" \"\n",
    "        print(sentence)\n",
    "    \n",
    "    # compare berttokenizer and keras tokenizer with inputs that basic-preprocessed\n",
    "compare_bert_and_keras_tokenizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51b7c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_bert_and_keras_tokenizer_w_basic_preprocessing(sentences):\n",
    "    # compare berttokenizer and keras tokenizer \n",
    "    start = 6666\n",
    "    no = 5000\n",
    "    kt = pp.tokenizer_init(X_train[start:(start+no)], X_test) # what should it be trained on? Fuck\n",
    "    bt = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    for i, s in enumerate(sentences):\n",
    "        print(f\"***{i}***\")\n",
    "        print(\"--Before\")\n",
    "        print(s)\n",
    "        \n",
    "        print(\"--After basic_preprocessing\")\n",
    "        print(s)\n",
    "        tokens = []\n",
    "        for token in pp.basic_preprocess(s.split()):\n",
    "            tokens.extend([t for t in token])\n",
    "        s_basic_preprocessed = \" \".join(tokens)\n",
    "        \n",
    "        print(\"--After bert tokenization with basic preprocessing\")\n",
    "        print(\" \".join(bt.tokenize(s_basic_preprocessed)))\n",
    "        print(\"--After keras tokenization with basic preprocessing\")\n",
    "        sequence = kt.texts_to_sequences(s_basic_preprocessed)\n",
    "        tokens = kt.sequences_to_texts(sequence)\n",
    "        sentence = \"\"\n",
    "        for t in tokens:\n",
    "            if t != \"\": sentence += t\n",
    "            else: sentence += \" \"\n",
    "        print(sentence)\n",
    "        \n",
    "        print(\"**********\")\n",
    "        print(\"**********\")\n",
    "        print(\"**********\")\n",
    "    \n",
    "    # compare berttokenizer and keras tokenizer with inputs that basic-preprocessed\n",
    "compare_bert_and_keras_tokenizer_w_basic_preprocessing(X_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1caf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffae52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817f108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3164369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3bde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a04de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
