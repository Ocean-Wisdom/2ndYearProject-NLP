{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b29b501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MUSIC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MUSIC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import imports as ii\n",
    "import functions as f\n",
    "import preprocessing as pp\n",
    "import neuralnetworks as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e684a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73776081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  100000\n",
      "Number of data:  10000\n"
     ]
    }
   ],
   "source": [
    "PATH = {}\n",
    "PATH[\"dataset_classification\"] = \"dataset/classification/\"\n",
    "PATH[\"dataset_labeling\"] = \"dataset/seq_labeling/\"\n",
    "PATH[\"music_reviews_train\"] = PATH[\"dataset_classification\"] + \"music_reviews_train.json.gz\"\n",
    "PATH[\"music_reviews_dev\"] = PATH[\"dataset_classification\"] + \"music_reviews_dev.json.gz\"\n",
    "PATH[\"music_reviews_test\"] = PATH[\"dataset_classification\"] + \"music_reviews_test_masked.json.gz\"\n",
    "train = f.readJson(PATH[\"music_reviews_train\"])\n",
    "test = f.readJson(PATH[\"music_reviews_dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a207328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, train_idx, train_missing_idx = f.json_divide(train)\n",
    "X_test, y_test, test_idx, test_missing_idx = f.json_divide(test)\n",
    "\n",
    "# convert labels\n",
    "sent_dict = {\"positive\": 1, \"negative\": 0}\n",
    "y_train = pp.sentiment_converter(y_train, sent_dict)\n",
    "y_test = pp.sentiment_converter(y_test, sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bac2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack\n",
    "end = 5000\n",
    "start = 100\n",
    "X_train = X_train[start:end+start]\n",
    "y_train = y_train[start:end+start]\n",
    "X_test = X_test[start:end+start]\n",
    "y_test = y_test[start:end+start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b1b22f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5966666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# accuracy score\n",
    "result = ii.np.round(model.predict(X_test)) # use model from the previous step\n",
    "print(i.accuracy_score(y_test, result))\n",
    "# plotting \n",
    "f.plot_model_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3318b",
   "metadata": {},
   "source": [
    "# POS-tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/api/nltk.tag.html\n",
    "from nltk import pos_tag, word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#only works for english\n",
    "def pos_tag_stringlist(strlist, shouldTokenize):\n",
    "    pos_tagged_strlist = []\n",
    "    if shouldTokenize: \n",
    "        for str in strlist: pos_tagged_strlist.append(pos_tag(word_tokenize(str)))\n",
    "    else: \n",
    "        for str in strlist: pos_tagged_strlist.append(pos_tag(str))\n",
    "    return pos_tagged_strlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbfe3c",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5962aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_list, test_list, y_train, y_test):\n",
    "    simp_contr = [0, 1]\n",
    "    gram_cor = [0, 1]\n",
    "    simp_neg = [0, 1]\n",
    "    lemma = [0, 1]\n",
    "    rem_stop = [0, 1]\n",
    "    basic_preprocessing = 1\n",
    "    list_of_data = []\n",
    "    for z in simp_contr:\n",
    "        for x in gram_cor:\n",
    "            for c in simp_neg:\n",
    "                for v in lemma:\n",
    "                    for b in rem_stop:\n",
    "                        train = train_list\n",
    "                        test = test_list\n",
    "                        if z == 1: # contractions\n",
    "                            train = f.simplify_contraction(train)\n",
    "                            test = f.simplify_contraction(test)\n",
    "                        if basic_preprocessing == 1: # basic preprocessing\n",
    "                            train = pp.basic_preprocess(train)\n",
    "                            test = pp.basic_preprocess(test)\n",
    "                        if x == 1: # grammar correction \n",
    "                            train = pp.grammar_corrector(train)\n",
    "                            test = pp.grammar_corrector(test)\n",
    "                        if c == 1: # Simnplyfy Negotiation \n",
    "                            train = f.simplify_negation(train)\n",
    "                            test = f.simplify_negation(test)\n",
    "                        if v == 1: # Lemmatize \n",
    "                            train = pp.lemmatize_sentencelist(train)\n",
    "                            test = pp.lemmatize_sentencelist(test)\n",
    "                        if b == 1: # Remove stop words\n",
    "                            train = pp.remove_stop_words(train)\n",
    "                            test = pp.remove_stop_words(test)\n",
    "\n",
    "                        list_of_data.append([[z, basic_preprocessing, x, c, v, b], train, test]) #\n",
    "    return list_of_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9650d",
   "metadata": {},
   "source": [
    "# Below line is running for a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8bedf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets, y_train, y_test = grid_search(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a9f8e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 8s 111ms/step - loss: 1.5828 - accuracy: 0.5970 - val_loss: 0.6787 - val_accuracy: 0.5880 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.7683\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.5142 - accuracy: 0.7683 - val_loss: 0.4495 - val_accuracy: 0.8110 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9220\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 8s 128ms/step - loss: 0.2268 - accuracy: 0.9220 - val_loss: 0.5056 - val_accuracy: 0.8150 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9417\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.1749 - accuracy: 0.9417 - val_loss: 0.4675 - val_accuracy: 0.8343 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9473\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 117ms/step - loss: 0.1535 - accuracy: 0.9473 - val_loss: 0.4567 - val_accuracy: 0.8337 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1496 - accuracy: 0.9497\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 119ms/step - loss: 0.1496 - accuracy: 0.9497 - val_loss: 0.4569 - val_accuracy: 0.8340 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 119ms/step - loss: 0.7984 - accuracy: 0.6143 - val_loss: 0.6827 - val_accuracy: 0.5817 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.7040\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.6415 - accuracy: 0.7040 - val_loss: 0.6796 - val_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.8840\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 114ms/step - loss: 0.2961 - accuracy: 0.8840 - val_loss: 0.5231 - val_accuracy: 0.8117 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9303\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2071 - accuracy: 0.9303 - val_loss: 0.5831 - val_accuracy: 0.8123 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9327\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.1925 - accuracy: 0.9327 - val_loss: 0.5902 - val_accuracy: 0.8137 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9330\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.1903 - accuracy: 0.9330 - val_loss: 0.5904 - val_accuracy: 0.8130 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9327\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.1898 - accuracy: 0.9327 - val_loss: 0.5904 - val_accuracy: 0.8130 - lr: 3.2000e-07\n",
      "Epoch 7: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 132ms/step - loss: 0.7233 - accuracy: 0.6223 - val_loss: 0.6781 - val_accuracy: 0.5903 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.7313\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 117ms/step - loss: 0.5656 - accuracy: 0.7313 - val_loss: 0.5771 - val_accuracy: 0.7540 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9003\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2477 - accuracy: 0.9003 - val_loss: 0.5708 - val_accuracy: 0.8397 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9373\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 6s 106ms/step - loss: 0.1714 - accuracy: 0.9373 - val_loss: 0.5187 - val_accuracy: 0.8470 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9440\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.1590 - accuracy: 0.9440 - val_loss: 0.5189 - val_accuracy: 0.8447 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9457\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 110ms/step - loss: 0.1556 - accuracy: 0.9457 - val_loss: 0.5220 - val_accuracy: 0.8447 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 8s 116ms/step - loss: 1.9028 - accuracy: 0.5673 - val_loss: 0.6908 - val_accuracy: 0.5763 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.6487\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 109ms/step - loss: 0.6079 - accuracy: 0.6487 - val_loss: 0.6837 - val_accuracy: 0.5860 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.7360\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.5138 - accuracy: 0.7360 - val_loss: 0.6660 - val_accuracy: 0.6440 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.8313\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.3635 - accuracy: 0.8313 - val_loss: 0.6017 - val_accuracy: 0.7690 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.8873\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.2841 - accuracy: 0.8873 - val_loss: 0.6088 - val_accuracy: 0.7817 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.8917\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2721 - accuracy: 0.8917 - val_loss: 0.6074 - val_accuracy: 0.7840 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.8927\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.2702 - accuracy: 0.8927 - val_loss: 0.6073 - val_accuracy: 0.7847 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.8930\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.2699 - accuracy: 0.8930 - val_loss: 0.6073 - val_accuracy: 0.7847 - lr: 6.4000e-08\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.8930\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.2698 - accuracy: 0.8930 - val_loss: 0.6073 - val_accuracy: 0.7847 - lr: 1.2800e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 8s 116ms/step - loss: 1.1532 - accuracy: 0.6017 - val_loss: 0.6915 - val_accuracy: 0.5837 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.6983\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 110ms/step - loss: 0.5731 - accuracy: 0.6983 - val_loss: 0.6501 - val_accuracy: 0.6327 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.8490\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 110ms/step - loss: 0.3511 - accuracy: 0.8490 - val_loss: 0.5343 - val_accuracy: 0.7980 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 0.9270\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.2141 - accuracy: 0.9270 - val_loss: 0.5513 - val_accuracy: 0.8230 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9313\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2029 - accuracy: 0.9313 - val_loss: 0.5543 - val_accuracy: 0.8247 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1998 - accuracy: 0.9330\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 110ms/step - loss: 0.1998 - accuracy: 0.9330 - val_loss: 0.5543 - val_accuracy: 0.8250 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1991 - accuracy: 0.9327\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 110ms/step - loss: 0.1991 - accuracy: 0.9327 - val_loss: 0.5543 - val_accuracy: 0.8250 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9327\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.1990 - accuracy: 0.9327 - val_loss: 0.5543 - val_accuracy: 0.8250 - lr: 6.4000e-08\n",
      "Epoch 8: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 10s 148ms/step - loss: 1.0937 - accuracy: 0.5743 - val_loss: 0.6887 - val_accuracy: 0.5753 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.6540\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 123ms/step - loss: 0.6037 - accuracy: 0.6540 - val_loss: 0.6886 - val_accuracy: 0.5813 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5596 - accuracy: 0.6910\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 8s 126ms/step - loss: 0.5596 - accuracy: 0.6910 - val_loss: 0.6937 - val_accuracy: 0.5880 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5454 - accuracy: 0.7110\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 124ms/step - loss: 0.5454 - accuracy: 0.7110 - val_loss: 0.6922 - val_accuracy: 0.5900 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.7143\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 125ms/step - loss: 0.5422 - accuracy: 0.7143 - val_loss: 0.6922 - val_accuracy: 0.5907 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.7150\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 124ms/step - loss: 0.5415 - accuracy: 0.7150 - val_loss: 0.6922 - val_accuracy: 0.5907 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5413 - accuracy: 0.7150\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 124ms/step - loss: 0.5413 - accuracy: 0.7150 - val_loss: 0.6922 - val_accuracy: 0.5907 - lr: 3.2000e-07\n",
      "Epoch 7: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 129ms/step - loss: 0.8834 - accuracy: 0.5887 - val_loss: 0.6806 - val_accuracy: 0.5853 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5779 - accuracy: 0.6970\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 0.5779 - accuracy: 0.6970 - val_loss: 0.6645 - val_accuracy: 0.6427 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.7933\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 122ms/step - loss: 0.4046 - accuracy: 0.7933 - val_loss: 0.4889 - val_accuracy: 0.7177 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.8760\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 122ms/step - loss: 0.2734 - accuracy: 0.8760 - val_loss: 0.4795 - val_accuracy: 0.7823 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.8993\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 123ms/step - loss: 0.2525 - accuracy: 0.8993 - val_loss: 0.4790 - val_accuracy: 0.7757 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9007\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 122ms/step - loss: 0.2478 - accuracy: 0.9007 - val_loss: 0.4784 - val_accuracy: 0.7773 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 125ms/step - loss: 0.7479 - accuracy: 0.6047 - val_loss: 0.6895 - val_accuracy: 0.5827 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.7473\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 118ms/step - loss: 0.5081 - accuracy: 0.7473 - val_loss: 0.5751 - val_accuracy: 0.7533 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9200\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 117ms/step - loss: 0.2349 - accuracy: 0.9200 - val_loss: 0.4918 - val_accuracy: 0.7837 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9423\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 118ms/step - loss: 0.1934 - accuracy: 0.9423 - val_loss: 0.4535 - val_accuracy: 0.8277 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9477\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 118ms/step - loss: 0.1721 - accuracy: 0.9477 - val_loss: 0.4582 - val_accuracy: 0.8297 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9480\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 119ms/step - loss: 0.1685 - accuracy: 0.9480 - val_loss: 0.4592 - val_accuracy: 0.8283 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9480\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 7s 118ms/step - loss: 0.1677 - accuracy: 0.9480 - val_loss: 0.4606 - val_accuracy: 0.8287 - lr: 3.2000e-07\n",
      "Epoch 7: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 120ms/step - loss: 0.7279 - accuracy: 0.6170 - val_loss: 0.6877 - val_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.7197\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.5485 - accuracy: 0.7197 - val_loss: 0.6159 - val_accuracy: 0.7650 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9097\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2461 - accuracy: 0.9097 - val_loss: 0.4719 - val_accuracy: 0.8260 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9373\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.1649 - accuracy: 0.9373 - val_loss: 0.4890 - val_accuracy: 0.8333 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9437\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.1520 - accuracy: 0.9437 - val_loss: 0.4922 - val_accuracy: 0.8320 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9440\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 120ms/step - loss: 0.1500 - accuracy: 0.9440 - val_loss: 0.4926 - val_accuracy: 0.8327 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 121ms/step - loss: 1.2307 - accuracy: 0.5690 - val_loss: 0.6913 - val_accuracy: 0.5737 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.6363\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 114ms/step - loss: 0.6157 - accuracy: 0.6363 - val_loss: 0.6795 - val_accuracy: 0.5867 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.7963\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.4193 - accuracy: 0.7963 - val_loss: 0.4961 - val_accuracy: 0.7977 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9237\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.2299 - accuracy: 0.9237 - val_loss: 0.4998 - val_accuracy: 0.7993 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9293\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 114ms/step - loss: 0.2015 - accuracy: 0.9293 - val_loss: 0.4999 - val_accuracy: 0.8017 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.9323\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.1969 - accuracy: 0.9323 - val_loss: 0.5015 - val_accuracy: 0.8020 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1960 - accuracy: 0.9320\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 117ms/step - loss: 0.1960 - accuracy: 0.9320 - val_loss: 0.5015 - val_accuracy: 0.8020 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9320\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.1958 - accuracy: 0.9320 - val_loss: 0.5015 - val_accuracy: 0.8020 - lr: 6.4000e-08\n",
      "Epoch 8: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 121ms/step - loss: 0.7194 - accuracy: 0.6277 - val_loss: 0.6777 - val_accuracy: 0.5920 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6122 - accuracy: 0.7227\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.6122 - accuracy: 0.7227 - val_loss: 0.6979 - val_accuracy: 0.6633 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.8453\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.3642 - accuracy: 0.8453 - val_loss: 0.5883 - val_accuracy: 0.8173 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.9117\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2492 - accuracy: 0.9117 - val_loss: 0.5894 - val_accuracy: 0.8157 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9183\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2312 - accuracy: 0.9183 - val_loss: 0.5859 - val_accuracy: 0.8183 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9187\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2282 - accuracy: 0.9187 - val_loss: 0.5866 - val_accuracy: 0.8180 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9187\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 114ms/step - loss: 0.2276 - accuracy: 0.9187 - val_loss: 0.5867 - val_accuracy: 0.8180 - lr: 3.2000e-07\n",
      "Epoch 7: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 122ms/step - loss: 1.2991 - accuracy: 0.5893 - val_loss: 0.6912 - val_accuracy: 0.5773 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6180 - accuracy: 0.6413\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.6180 - accuracy: 0.6413 - val_loss: 0.6930 - val_accuracy: 0.5883 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.7337\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.5213 - accuracy: 0.7337 - val_loss: 0.6812 - val_accuracy: 0.7700 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.8807\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.3212 - accuracy: 0.8807 - val_loss: 0.6416 - val_accuracy: 0.7907 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.8887\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.2903 - accuracy: 0.8887 - val_loss: 0.6383 - val_accuracy: 0.7907 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8943\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.2856 - accuracy: 0.8943 - val_loss: 0.6378 - val_accuracy: 0.7903 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 122ms/step - loss: 0.7596 - accuracy: 0.6263 - val_loss: 0.6841 - val_accuracy: 0.5797 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.7013\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.5680 - accuracy: 0.7013 - val_loss: 0.7243 - val_accuracy: 0.7333 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.9000\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.2832 - accuracy: 0.9000 - val_loss: 0.6115 - val_accuracy: 0.8260 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9327\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.1939 - accuracy: 0.9327 - val_loss: 0.5871 - val_accuracy: 0.8320 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9403\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 117ms/step - loss: 0.1770 - accuracy: 0.9403 - val_loss: 0.5628 - val_accuracy: 0.8333 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9423\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.1739 - accuracy: 0.9423 - val_loss: 0.5637 - val_accuracy: 0.8337 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9430\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.1734 - accuracy: 0.9430 - val_loss: 0.5639 - val_accuracy: 0.8337 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9430\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.1733 - accuracy: 0.9430 - val_loss: 0.5639 - val_accuracy: 0.8337 - lr: 6.4000e-08\n",
      "Epoch 8: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 123ms/step - loss: 0.7472 - accuracy: 0.6020 - val_loss: 0.6921 - val_accuracy: 0.5747 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.6553\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.6324 - accuracy: 0.6553 - val_loss: 0.9247 - val_accuracy: 0.7473 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8580\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.3420 - accuracy: 0.8580 - val_loss: 0.5967 - val_accuracy: 0.7493 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9197\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.2369 - accuracy: 0.9197 - val_loss: 0.5870 - val_accuracy: 0.8040 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9253\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.2064 - accuracy: 0.9253 - val_loss: 0.5854 - val_accuracy: 0.8103 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9247\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.2035 - accuracy: 0.9247 - val_loss: 0.5859 - val_accuracy: 0.8107 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9250\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.2029 - accuracy: 0.9250 - val_loss: 0.5860 - val_accuracy: 0.8100 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9250\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.2028 - accuracy: 0.9250 - val_loss: 0.5860 - val_accuracy: 0.8100 - lr: 6.4000e-08\n",
      "Epoch 8: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 8s 118ms/step - loss: 1.4234 - accuracy: 0.6060 - val_loss: 0.6886 - val_accuracy: 0.5997 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.7237\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.5559 - accuracy: 0.7237 - val_loss: 0.6489 - val_accuracy: 0.6547 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.8097\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.4000 - accuracy: 0.8097 - val_loss: 0.5868 - val_accuracy: 0.7323 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8730\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.3066 - accuracy: 0.8730 - val_loss: 0.5505 - val_accuracy: 0.7607 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8883\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2759 - accuracy: 0.8883 - val_loss: 0.5483 - val_accuracy: 0.7683 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.8927\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.2695 - accuracy: 0.8927 - val_loss: 0.5488 - val_accuracy: 0.7703 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.8940\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.2683 - accuracy: 0.8940 - val_loss: 0.5484 - val_accuracy: 0.7707 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.8940\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.2681 - accuracy: 0.8940 - val_loss: 0.5484 - val_accuracy: 0.7707 - lr: 6.4000e-08\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.8940\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "60/60 [==============================] - 7s 111ms/step - loss: 0.2680 - accuracy: 0.8940 - val_loss: 0.5483 - val_accuracy: 0.7707 - lr: 1.2800e-08\n",
      "Epoch 9: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 120ms/step - loss: 1.4930 - accuracy: 0.5873 - val_loss: 0.6915 - val_accuracy: 0.5780 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5855 - accuracy: 0.6747\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.5855 - accuracy: 0.6747 - val_loss: 0.8197 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8853\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 117ms/step - loss: 0.3735 - accuracy: 0.8853 - val_loss: 0.5397 - val_accuracy: 0.7773 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9197\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 109ms/step - loss: 0.2417 - accuracy: 0.9197 - val_loss: 0.5417 - val_accuracy: 0.8100 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9277\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 109ms/step - loss: 0.2152 - accuracy: 0.9277 - val_loss: 0.5416 - val_accuracy: 0.8127 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9277\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 6s 108ms/step - loss: 0.2110 - accuracy: 0.9277 - val_loss: 0.5445 - val_accuracy: 0.8130 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.9277\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 6s 108ms/step - loss: 0.2101 - accuracy: 0.9277 - val_loss: 0.5446 - val_accuracy: 0.8133 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9277\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 6s 108ms/step - loss: 0.2100 - accuracy: 0.9277 - val_loss: 0.5446 - val_accuracy: 0.8133 - lr: 6.4000e-08\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9277\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "60/60 [==============================] - 6s 108ms/step - loss: 0.2099 - accuracy: 0.9277 - val_loss: 0.5446 - val_accuracy: 0.8133 - lr: 1.2800e-08\n",
      "Epoch 9: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 123ms/step - loss: 0.8609 - accuracy: 0.6177 - val_loss: 0.6859 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.6927\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 116ms/step - loss: 0.5796 - accuracy: 0.6927 - val_loss: 0.6611 - val_accuracy: 0.6283 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8290\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 115ms/step - loss: 0.3671 - accuracy: 0.8290 - val_loss: 0.5949 - val_accuracy: 0.8210 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9160\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 118ms/step - loss: 0.2181 - accuracy: 0.9160 - val_loss: 0.5415 - val_accuracy: 0.8323 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9240\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 122ms/step - loss: 0.1973 - accuracy: 0.9240 - val_loss: 0.5450 - val_accuracy: 0.8323 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9257\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 8s 126ms/step - loss: 0.1935 - accuracy: 0.9257 - val_loss: 0.5441 - val_accuracy: 0.8333 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9263\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 8s 134ms/step - loss: 0.1928 - accuracy: 0.9263 - val_loss: 0.5441 - val_accuracy: 0.8333 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9263\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 7s 124ms/step - loss: 0.1927 - accuracy: 0.9263 - val_loss: 0.5441 - val_accuracy: 0.8333 - lr: 6.4000e-08\n",
      "Epoch 8: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 10s 131ms/step - loss: 0.7373 - accuracy: 0.6117 - val_loss: 0.6834 - val_accuracy: 0.5787 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5740 - accuracy: 0.6767\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 118ms/step - loss: 0.5740 - accuracy: 0.6767 - val_loss: 0.6195 - val_accuracy: 0.6107 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.8467\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 120ms/step - loss: 0.3450 - accuracy: 0.8467 - val_loss: 0.5087 - val_accuracy: 0.8020 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9207\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 8s 130ms/step - loss: 0.2113 - accuracy: 0.9207 - val_loss: 0.5358 - val_accuracy: 0.8063 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9257\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 125ms/step - loss: 0.1937 - accuracy: 0.9257 - val_loss: 0.5405 - val_accuracy: 0.8097 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9283\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 119ms/step - loss: 0.1897 - accuracy: 0.9283 - val_loss: 0.5404 - val_accuracy: 0.8093 - lr: 1.6000e-06\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9293\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 0.1890 - accuracy: 0.9293 - val_loss: 0.5402 - val_accuracy: 0.8090 - lr: 3.2000e-07\n",
      "Epoch 7: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 122ms/step - loss: 0.9663 - accuracy: 0.6110 - val_loss: 0.6973 - val_accuracy: 0.5813 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.6607\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 8s 127ms/step - loss: 0.5872 - accuracy: 0.6607 - val_loss: 0.6865 - val_accuracy: 0.6070 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.7500\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 120ms/step - loss: 0.5381 - accuracy: 0.7500 - val_loss: 0.6845 - val_accuracy: 0.6200 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.7700\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 125ms/step - loss: 0.5166 - accuracy: 0.7700 - val_loss: 0.6815 - val_accuracy: 0.6260 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.7747\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 8s 141ms/step - loss: 0.5093 - accuracy: 0.7747 - val_loss: 0.6803 - val_accuracy: 0.6277 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.7753\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 9s 144ms/step - loss: 0.5077 - accuracy: 0.7753 - val_loss: 0.6800 - val_accuracy: 0.6277 - lr: 1.6000e-06\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.7750\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "60/60 [==============================] - 8s 132ms/step - loss: 0.5073 - accuracy: 0.7750 - val_loss: 0.6800 - val_accuracy: 0.6283 - lr: 3.2000e-07\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.7750\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "60/60 [==============================] - 8s 127ms/step - loss: 0.5073 - accuracy: 0.7750 - val_loss: 0.6800 - val_accuracy: 0.6283 - lr: 6.4000e-08\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.7750\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "60/60 [==============================] - 8s 129ms/step - loss: 0.5072 - accuracy: 0.7750 - val_loss: 0.6800 - val_accuracy: 0.6283 - lr: 1.2800e-08\n",
      "Epoch 9: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 11s 138ms/step - loss: 8.5917 - accuracy: 0.4430 - val_loss: 8.9053 - val_accuracy: 0.4227 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 8.5917 - accuracy: 0.4430\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 9s 156ms/step - loss: 8.5917 - accuracy: 0.4430 - val_loss: 8.9053 - val_accuracy: 0.4227 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 8.5917 - accuracy: 0.4430\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 125ms/step - loss: 8.5917 - accuracy: 0.4430 - val_loss: 8.9053 - val_accuracy: 0.4227 - lr: 2.0000e-04\n",
      "Epoch 3: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 10s 134ms/step - loss: 0.9723 - accuracy: 0.6123 - val_loss: 0.6862 - val_accuracy: 0.5810 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.6790\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 120ms/step - loss: 0.5836 - accuracy: 0.6790 - val_loss: 0.6808 - val_accuracy: 0.6283 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8240\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 122ms/step - loss: 0.4021 - accuracy: 0.8240 - val_loss: 0.5878 - val_accuracy: 0.8133 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9157\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 0.2279 - accuracy: 0.9157 - val_loss: 0.5342 - val_accuracy: 0.8223 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9243\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 0.2062 - accuracy: 0.9243 - val_loss: 0.5468 - val_accuracy: 0.8207 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2023 - accuracy: 0.9233\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 123ms/step - loss: 0.2023 - accuracy: 0.9233 - val_loss: 0.5485 - val_accuracy: 0.8217 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 131ms/step - loss: 1.1146 - accuracy: 0.5837 - val_loss: 0.6856 - val_accuracy: 0.5793 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.7097\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 122ms/step - loss: 0.5695 - accuracy: 0.7097 - val_loss: 0.6805 - val_accuracy: 0.7580 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.8957\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 120ms/step - loss: 0.2865 - accuracy: 0.8957 - val_loss: 0.5483 - val_accuracy: 0.8150 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.9287\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 123ms/step - loss: 0.2119 - accuracy: 0.9287 - val_loss: 0.5618 - val_accuracy: 0.8147 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9333\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 0.2005 - accuracy: 0.9333 - val_loss: 0.5590 - val_accuracy: 0.8140 - lr: 8.0000e-06\n",
      "Epoch 5: early stopping\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 9s 134ms/step - loss: 1.1046 - accuracy: 0.6180 - val_loss: 0.6849 - val_accuracy: 0.5937 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.7470\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60/60 [==============================] - 7s 125ms/step - loss: 0.5467 - accuracy: 0.7470 - val_loss: 0.5839 - val_accuracy: 0.7827 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9033\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "60/60 [==============================] - 7s 120ms/step - loss: 0.2823 - accuracy: 0.9033 - val_loss: 0.5203 - val_accuracy: 0.8340 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9363\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 0.2130 - accuracy: 0.9363 - val_loss: 0.5156 - val_accuracy: 0.8353 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9353\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "60/60 [==============================] - 7s 125ms/step - loss: 0.2006 - accuracy: 0.9353 - val_loss: 0.5235 - val_accuracy: 0.8347 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9347\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "60/60 [==============================] - 7s 124ms/step - loss: 0.1986 - accuracy: 0.9347 - val_loss: 0.5240 - val_accuracy: 0.8347 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x000001F78D11F0D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\music\\appdata\\local\\programs\\python\\python39\\lib\\weakref.py\", line 368, in remove\n",
      "    self = selfref()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/60 [====================>.........] - ETA: 1s - loss: 0.9479 - accuracy: 0.5905"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6692/1663367660.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# TRAIN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#print(\"shapes: \", X_train_p.shape, X_test_p.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRNN_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m      \u001b[1;31m### LOGGING INIT - RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# GET the index of the highest test ACCURACY where the RNN model stopped to TRAIN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ITU-BOOK\\2ndYearProject-NLP\\neuralnetworks.py\u001b[0m in \u001b[0;36mRNN_train\u001b[1;34m(X_train_p, y_train, X_test, y_test, tokenizer, maxlen)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     history = model.fit(X_train_p, y_train, validation_data=(X_test, y_test), \n\u001b[0m\u001b[0;32m     51\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### INIT RESULTS\n",
    "results, time = f.init_log_for_training()\n",
    "# Init past results, read in only the oldest file\n",
    "\n",
    "for data_set in data_sets:\n",
    "    # Tokenizer\n",
    "    labels = data_set[0]\n",
    "    # check whenever combination is already checked. Working only with RNN!:\n",
    "\n",
    "    tokenizer = pp.tokenizer_init(data_set[1], data_set[2])\n",
    "    Train = tokenizer.texts_to_sequences(data_set[1])\n",
    "    Test = tokenizer.texts_to_sequences(data_set[2])\n",
    "        # Sequencer \n",
    "    X_train_p = pp.sequence_pad(Train) # there are several attributes which can be defined, basic = first 50 words \n",
    "    X_test_p = pp.sequence_pad(Test)\n",
    "        \n",
    "        # TRAIN\n",
    "    #print(\"shapes: \", X_train_p.shape, X_test_p.shape)\n",
    "    history, model = nn.RNN_train(X_train_p, y_train, X_test_p, y_test, tokenizer)\n",
    "     ### LOGGING INIT - RNN\n",
    "    # GET the index of the highest test ACCURACY where the RNN model stopped to TRAIN\n",
    "    max_value = max(history.history['val_accuracy'])\n",
    "    max_index = history.history['val_accuracy'].index(max_value)\n",
    "    # append\n",
    "    \n",
    "    new_row = {'Running ID':time, \n",
    "           \"Model Name\":\"RNN\", \n",
    "          \"Expand Contractions\":labels[0],\n",
    "          \"Basic Preprocessing\":labels[1],\n",
    "          \"Grammar Correction\":labels[2],\n",
    "           \"Simplify Negotiations\": labels[3],\n",
    "          \"Lemmatize\": labels[4],\n",
    "          \"Remove Stop Words\": labels[5],\n",
    "          \"No. of Sentences\": len(data_set[1]),\n",
    "          \"Train Accuracy STOP\": history.history['accuracy'][max_index],\n",
    "          \"Test Accuracy STOP\": history.history['val_accuracy'][max_index],\n",
    "          \"Train Loss STOP\": history.history['loss'][max_index],\n",
    "          \"Test Loss STOP\": history.history['val_loss'][max_index]}\n",
    "    \n",
    "    results = results.append(new_row, ignore_index=True)\n",
    "    # maybe we dont need it in every round but how knows\n",
    "    try:\n",
    "        results.to_csv(\"results/results_\"+time+\".csv\")\n",
    "    except: \n",
    "        continue\n",
    "    f.plot_model_history(history)    \n",
    "    # CLEAN\n",
    "    del labels\n",
    "    del tokenizer\n",
    "    del Train\n",
    "    del Test\n",
    "    del X_train_p\n",
    "    del X_test_p\n",
    "    del history\n",
    "    del model \n",
    "    \n",
    "# save results again\n",
    "print(\"Combinations were checked\")\n",
    "results.to_csv(\"results/results_\"+time+\".csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed1bb9",
   "metadata": {},
   "source": [
    "# Leave it in here for experimenting purpose: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a57f5ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Combination\n",
      ":  [0, 1, 0, 0, 0, 0]\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 5s 175ms/step - loss: 1.2002 - accuracy: 0.5320 - val_loss: 0.6855 - val_accuracy: 0.5870 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6491 - accuracy: 0.5910\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "20/20 [==============================] - 3s 149ms/step - loss: 0.6491 - accuracy: 0.5910 - val_loss: 0.6657 - val_accuracy: 0.6240 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.7610\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "20/20 [==============================] - 3s 149ms/step - loss: 0.6129 - accuracy: 0.7610 - val_loss: 0.6637 - val_accuracy: 0.6260 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6038 - accuracy: 0.7640\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "20/20 [==============================] - 3s 150ms/step - loss: 0.6038 - accuracy: 0.7640 - val_loss: 0.6633 - val_accuracy: 0.6280 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6018 - accuracy: 0.7680\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "20/20 [==============================] - 3s 150ms/step - loss: 0.6018 - accuracy: 0.7680 - val_loss: 0.6632 - val_accuracy: 0.6270 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6013 - accuracy: 0.7680\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "20/20 [==============================] - 3s 149ms/step - loss: 0.6013 - accuracy: 0.7680 - val_loss: 0.6631 - val_accuracy: 0.6270 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n",
      "##### Combination\n",
      ":  [0, 1, 0, 0, 0, 1]\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 5s 176ms/step - loss: 1.6578 - accuracy: 0.5050 - val_loss: 0.6944 - val_accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6666 - accuracy: 0.5680\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "20/20 [==============================] - 3s 148ms/step - loss: 0.6666 - accuracy: 0.5680 - val_loss: 0.6800 - val_accuracy: 0.5850 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6259 - accuracy: 0.6930\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "20/20 [==============================] - 3s 148ms/step - loss: 0.6259 - accuracy: 0.6930 - val_loss: 0.6776 - val_accuracy: 0.5860 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6167 - accuracy: 0.6990\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "20/20 [==============================] - 3s 149ms/step - loss: 0.6167 - accuracy: 0.6990 - val_loss: 0.6771 - val_accuracy: 0.5900 - lr: 4.0000e-05\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6143 - accuracy: 0.6990\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "20/20 [==============================] - 3s 149ms/step - loss: 0.6143 - accuracy: 0.6990 - val_loss: 0.6770 - val_accuracy: 0.5900 - lr: 8.0000e-06\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.7000\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "20/20 [==============================] - 3s 148ms/step - loss: 0.6138 - accuracy: 0.7000 - val_loss: 0.6770 - val_accuracy: 0.5900 - lr: 1.6000e-06\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "'''### INIT RESULTS\n",
    "results, time = f.init_log_for_training()\n",
    "for c in range(2):\n",
    "    # Tokenizer\n",
    "    labels = data_sets[c][0]\n",
    "    print(\"##### Combination\\n: \", data_sets[c][0])\n",
    "    tokenizer = pp.tokenizer_init(data_sets[c][1], data_sets[c][2])\n",
    "    Train = tokenizer.texts_to_sequences(data_sets[c][1])\n",
    "    Test = tokenizer.texts_to_sequences(data_sets[c][2])\n",
    "        # Sequencer \n",
    "    X_train_p = pp.sequence_pad(Train) # there are several attributes which can be defined, basic = first 50 words \n",
    "    X_test_p = pp.sequence_pad(Test)\n",
    "        \n",
    "    # TRAIN RNN\n",
    "    history, model = nn.RNN_train(X_train_p, y_train, X_test_p, y_test, tokenizer)\n",
    "\n",
    "    ### LOGGING INIT - RNN\n",
    "    # GET the index of the highest test ACCURACY where the RNN model stopped to TRAIN\n",
    "    max_value = max(history.history['val_accuracy'])\n",
    "    max_index = history.history['val_accuracy'].index(max_value)\n",
    "    # append\n",
    "    \n",
    "    new_row = {'Running ID':save_time, \n",
    "           \"Model Name\":\"RNN\", \n",
    "          \"Expand Contractions\":labels[0],\n",
    "          \"Basic Preprocessing\":labels[1],\n",
    "          \"Grammar Correction\":labels[2],\n",
    "           \"Simplify Negotiations\": labels[3],\n",
    "          \"Lemmatize\": labels[4],\n",
    "          \"Remove Stop Words\": labels[5],\n",
    "          \"No. of Sentences\": len(data_sets[c][1]),\n",
    "          \"Train Accuracy STOP\": history.history['accuracy'][max_index],\n",
    "          \"Test Accuracy STOP\": history.history['val_accuracy'][max_index],\n",
    "          \"Train Loss STOP\": history.history['loss'][max_index],\n",
    "          \"Test Loss STOP\": history.history['val_loss'][max_index]}\n",
    "    \n",
    "    results = results.append(new_row, ignore_index=True)\n",
    "    \n",
    "# save results\n",
    "results.to_csv(\"results/results_\"+time+\".csv\") '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
