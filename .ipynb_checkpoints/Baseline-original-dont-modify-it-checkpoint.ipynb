{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37445fb",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9acc6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our functions\n",
    "import functions as f\n",
    "# readers\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# NN\n",
    "## early stopping\n",
    "import h5py\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# time\n",
    "from datetime import datetime\n",
    "\n",
    "# preprocessing\n",
    "import re\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13e1af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {}\n",
    "PATH[\"dataset_classification\"] = \"dataset/classification/\"\n",
    "PATH[\"dataset_labeling\"] = \"dataset/seq_labeling/\"\n",
    "PATH[\"music_reviews_train\"] = PATH[\"dataset_classification\"] + \"music_reviews_train.json.gz\"\n",
    "PATH[\"music_reviews_dev\"] = PATH[\"dataset_classification\"] + \"music_reviews_dev.json.gz\"\n",
    "PATH[\"music_reviews_test\"] = PATH[\"dataset_classification\"] + \"music_reviews_test_masked.json.gz\"\n",
    "\n",
    "# sent dict\n",
    "sent_dict = {\"positive\": 1, \"negative\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6a4e9",
   "metadata": {},
   "source": [
    "# 2. Read in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9bae5b53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  100000\n",
      "Number of data:  10000\n",
      "Number of data:  10000\n"
     ]
    }
   ],
   "source": [
    "train = f.readJson(PATH[\"music_reviews_train\"])\n",
    "dev = f.readJson(PATH[\"music_reviews_dev\"])\n",
    "test = f.readJson(PATH[\"music_reviews_test\"])\n",
    "length_of_sentencies_counter = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7339b8",
   "metadata": {},
   "source": [
    "## DELETE STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddb2660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to delete punctuations and some stop words\n",
    "import string\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    my_stop_words = ['$',\"'\",\"``\",\"''\",\"'s\"]\n",
    "    whitespace = [' ', '\\t', '\\n', '\\r', '\\x0b', '\\x0c']\n",
    "    punct = ['\"',\n",
    " '#',\n",
    " '$',\n",
    " '%',\n",
    " '&',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '*',\n",
    " '+',\n",
    " ',',\n",
    " '-',\n",
    " '/',\n",
    " ':',\n",
    " ';',\n",
    " '<',\n",
    " '=',\n",
    " '>',\n",
    " '@',\n",
    " '[',\n",
    " '\\\\',\n",
    " ']',\n",
    " '^',\n",
    " '_',\n",
    " '`',\n",
    " '{',\n",
    " '|',\n",
    " '}',\n",
    " '~'] # ? ! . are deleted \n",
    "\n",
    "    stop_words = set(['heee','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', '\\n', 'the', '\\x1bthis'] + list(my_stop_words) + list(string.digits) + punct)\n",
    "    clean_text = []\n",
    "    length_of_sentencies_counter = []\n",
    "    for sent in text:\n",
    "        # add whitespaces between punctuations, etc to be able to remove them\n",
    "        #print(sent)\n",
    "        #sent = re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', sent)\n",
    "        sent = re.sub('(?<! )(?=[.,!?()~${}\"|#&%*@\\'^+-/_0123456789:>`<;=\\[\\]])|(?<=[.,!?()~${}\"|#&%*@\\'^+-/_0123456789:>`<;=\\[\\]])(?! )', r' ', sent)\n",
    "        #sent = re.sub('([.,!?()])', r' \\1 ', sent)\n",
    "        d_sent = []\n",
    "        for c in sent.split():\n",
    "            if c in stop_words:\n",
    "                None\n",
    "            else:\n",
    "                d_sent.append(c)\n",
    "        clean_text.append(list(d_sent))\n",
    "        length_of_sentencies_counter.append(len(d_sent))\n",
    "        #print(d_sent)\n",
    "    return clean_text,length_of_sentencies_counter #length_of_sentencies_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d7020b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4603, 4838, 16019, 18229, 19785, 23315, 28397, 28834, 33359, 43138, 43539, 43690, 44008, 44477, 44972, 48811, 49317, 50106, 51717, 52286, 55555, 56171, 57223, 58328, 58799, 58866, 59525, 59739, 61046, 61914, 61916, 62831, 63208, 72268, 78944, 79067, 80093, 80637, 80658, 81640, 81900, 82510, 83138, 83145, 83615, 84761, 87112, 88895, 88984, 89132, 91949, 94301, 94727, 99641]\n",
      "##### \n"
     ]
    }
   ],
   "source": [
    "# read the train data\n",
    "data = train\n",
    "train_sent = []\n",
    "train_sentiment = []\n",
    "train_idx = []\n",
    "missing_indexies = []\n",
    "y_train = []\n",
    "length_of_sentencies_counter = []\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        train_sent.append(data[i][\"reviewText\"])\n",
    "        train_sentiment.append(data[i][\"sentiment\"])\n",
    "        train_idx.append(i)\n",
    "        y_train.append(sent_dict[data[i][\"sentiment\"]])\n",
    "        #length_of_sentencies_counter.append(len(data[i][\"reviewText\"].split()))\n",
    "    except KeyError:\n",
    "        missing_indexies.append(i)\n",
    "        continue\n",
    "print(missing_indexies)\n",
    "\n",
    "########################## remove stop words delet missing shit \n",
    "##### ------------- remove stop words\n",
    "train_sent,length_of_sentencies_counter = remove_stop_words(train_sent)\n",
    "print(\"##### \")\n",
    "empties = []\n",
    "for i in range(len(train_sent)):\n",
    "    if len(train_sent[i]) == 0:\n",
    "        empties.append(i)\n",
    "c = 0\n",
    "for i in empties:\n",
    "    #print(train_sent[i-c])\n",
    "    del train_sent[i-c]\n",
    "    del train_sentiment[i-c]\n",
    "    del y_train[i-c]\n",
    "    c += 1\n",
    "    #print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f695589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3239\n",
      "0\n",
      "average number of words in the sentence\n",
      "32.08572629219778\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train data reviews length simple statistic\n",
    "import matplotlib.pyplot as plt\n",
    "print(max(length_of_sentencies_counter))\n",
    "print(min(length_of_sentencies_counter))\n",
    "bins = [10, 20, 30,40,50,70,80,90,100,150,200,300,400,500,600]\n",
    "plt.hist(length_of_sentencies_counter, bins = range(0, 200, 10))\n",
    "print(\"average number of words in the sentence\")\n",
    "print(np.average(length_of_sentencies_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47903531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[2900, 4294, 5135, 8540]\n",
      "##### \n",
      "9984\n",
      "9984\n",
      "9984\n"
     ]
    }
   ],
   "source": [
    "data = dev\n",
    "print(len(dev))\n",
    "dev_sent = []\n",
    "dev_sentiment = []\n",
    "dev_idx = []\n",
    "missing_indexies = []\n",
    "dev_y_train = []\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        dev_sent.append(data[i][\"reviewText\"])\n",
    "        dev_sentiment.append(data[i][\"sentiment\"])\n",
    "        dev_idx.append(i)\n",
    "        dev_y_train.append(sent_dict[data[i][\"sentiment\"]]) \n",
    "    except KeyError:\n",
    "        missing_indexies.append(i)\n",
    "        continue\n",
    "print(missing_indexies)\n",
    "\n",
    "# remove stop words\n",
    "dev_sent, l = remove_stop_words(dev_sent)\n",
    "\n",
    "########################## remove stop words delet missing shit \n",
    "##### ------------- remove stop words\n",
    "\n",
    "print(\"##### \")\n",
    "empties = []\n",
    "for i in range(len(dev_sent)):\n",
    "    if len(dev_sent[i]) == 0:\n",
    "        empties.append(i)\n",
    "c = 0\n",
    "for i in empties:\n",
    "    #print(train_sent[i-c])\n",
    "    del dev_sent[i-c]\n",
    "    del dev_sentiment[i-c]\n",
    "    del dev_y_train[i-c]\n",
    "    c += 1\n",
    "    #print(c)\n",
    "print(len(dev_sent))\n",
    "print(len(dev_sentiment))\n",
    "print(len(dev_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d15972ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of reviews which are empty:  [90, 117, 439, 2138, 4112, 5364, 8210]\n"
     ]
    }
   ],
   "source": [
    "data = test\n",
    "test_sent = []\n",
    "test_sentiment = []\n",
    "test_idx = []\n",
    "test_missing_indexies = []\n",
    "#print(test)\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        test_sent.append(data[i][\"reviewText\"])\n",
    "        test_sentiment.append(data[i][\"sentiment\"])\n",
    "        #print(data[i][\"sentiment\"])\n",
    "        test_idx.append(i)\n",
    "        #y_train.append(sent_dict[data[i][\"sentiment\"]]) \n",
    "    except KeyError:\n",
    "        test_missing_indexies.append(i)\n",
    "        continue\n",
    "print(\"index of reviews which are empty: \",test_missing_indexies)\n",
    "\n",
    "# remove stop words\n",
    "test_sent,l = remove_stop_words(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9014e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\music\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\zeugma\\keras_transformers.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(self.texts_to_sequences(texts))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum words:  68526\n"
     ]
    }
   ],
   "source": [
    "# it needs to be replaced since it doesnt handle the lower/upper cases and the mess\n",
    "###################################################################################\n",
    "from zeugma import TextsToSequences\n",
    "sequencer = TextsToSequences()\n",
    "\n",
    "sequencer.fit_transform(train_sent)\n",
    "# save the sequencer  than load it again\n",
    "print(\"maximum words: \", max(sequencer.index_word))\n",
    "import pickle\n",
    "b = pickle.dumps(sequencer)\n",
    "#pickle.dump( b, open( \"model/text_to_seq.p\", \"wb\" ) )\n",
    "pickle.dump( b, open( \"model/text_to_seq_without_stop_words.p\", \"wb\" ) )\n",
    "# load \n",
    "#sequencer = pickle.load( open( \"model/text_to_seq.p\", \"rb\" ) )\n",
    "sequencer = pickle.load( open( \"model/text_to_seq_without_stop_words.p\", \"rb\" ) )\n",
    "sequencer = pickle.loads(sequencer)\n",
    "\n",
    "#print(sequencer.index_word)\n",
    "# fit new data \n",
    "Train = sequencer.transform(train_sent)\n",
    "Dev = sequencer.transform(dev_sent)\n",
    "Test = sequencer.transform(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d28e1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "083495a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_pad(sequence, padding='pre', maxlen=50):\n",
    "    res = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=maxlen,\n",
    "        dtype='int32',\n",
    "        padding=padding,\n",
    "        truncating='pre',\n",
    "        value=0.0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436426ae",
   "metadata": {},
   "source": [
    "## Padding the dataset, POST padding works better a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e851026c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99800"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = \"post\"\n",
    "maxlen = 60 # how many words from the review\n",
    "X_train_p = sequence_pad(Train, padding=padding, maxlen=maxlen)\n",
    "X_dev_p = sequence_pad(Dev, padding=padding, maxlen=maxlen)\n",
    "X_test_p = sequence_pad(Test, padding=padding, maxlen=maxlen)\n",
    "len(X_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "83330e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99800\n",
      "99800\n",
      "9984\n",
      "9984\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "X_train_m = np.array(X_train_p)\n",
    "y_train_m = np.array(y_train)\n",
    "print(len(y_train_m))\n",
    "print(len(X_train_m))\n",
    "X_valid_m = np.array(X_dev_p)\n",
    "y_valid_m = np.array(dev_y_train)\n",
    "print(len(X_valid_m))\n",
    "print(len(y_valid_m))\n",
    "X_test_m = np.array(X_test_p)\n",
    "#y_test_m = np.array(y_test)\n",
    "#print(len(X_test_m))\n",
    "#print(len(y_test_m))\n",
    "\n",
    "# sampling some random data for tran and DEV\n",
    "#random.seed(40)\n",
    "bias = 50000\n",
    "\n",
    "ran = random.randint(0, len(X_train_m)-bias)\n",
    "ran_val = random.randint(0, len(X_valid_m)-(bias/10))\n",
    "s_tr = ran\n",
    "e = ran+bias\n",
    "s_test = ran_val\n",
    "e_test = ran_val+(bias/10)\n",
    "\n",
    "X_train_m = X_train_m[s_tr:e]\n",
    "y_train_m = y_train_m[s_tr:e]\n",
    "#X_valid_m = X_valid_m[s_test:int(e_test)]\n",
    "#y_valid_m = y_valid_m[s_test:int(e_test)]\n",
    "#X_valid_m = X_valid_m[s:e]\n",
    "#y_valid_m = y_valid_m[s:e]\n",
    "print(len(y_train_m))\n",
    "print(len(X_train_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e8706",
   "metadata": {},
   "source": [
    "# Early stopping Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "65896bde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_27 (Embedding)    (None, 60, 100)           6852700   \n",
      "                                                                 \n",
      " lstm_27 (LSTM)              (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,933,201\n",
      "Trainable params: 6,933,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 193s 191ms/step - loss: 0.3553 - accuracy: 0.8552 - val_loss: 0.3262 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.2243 - accuracy: 0.9212 - val_loss: 0.3254 - val_accuracy: 0.8767 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.1746 - accuracy: 0.9403 - val_loss: 0.3915 - val_accuracy: 0.8668 - lr: 0.0010\n",
      "Epoch 4/20\n",
      " 644/1000 [==================>...........] - ETA: 49s - loss: 0.1471 - accuracy: 0.9521"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9208/735562277.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#X_valid, y_valid = X_train_k[batch_size:], y_train[batch_size:]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m history = model.fit(X_train_m, y_train_m, validation_data=(X_valid_m, y_valid_m), \n\u001b[0m\u001b[0;32m     38\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocabulary_size = max(sequencer.index_word)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# get time for saving the model\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "save_time = str(dateTimeObj.year)+'-'+str(dateTimeObj.month)+'-'+str(dateTimeObj.day)+'-'+str(dateTimeObj.hour)+'-'+str(dateTimeObj.minute)+'-'+str(dateTimeObj.second)\n",
    "\n",
    "# define the model\n",
    "\n",
    "embedding_size=64\n",
    "embedding_size=100 # bigger = slower train\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size+1, embedding_size, input_length=maxlen))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid')) # shape of the labels, if its 2, than the y_ labels has a x* 2 shape \n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# early stopping\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('model/'+save_time+'-model.mdl_wts.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, verbose=1, mode='min')\n",
    "# patience: 10% of number of epochs. Anyway, it is just for stopping the validation, since we have model checkpoint its doesnt matter\n",
    "batch_size = 50 # lower = slower train, higher = faster train\n",
    "num_epochs = 20\n",
    "\n",
    "#X_valid, y_valid = X_train_k[batch_size:], y_train[batch_size:]\n",
    "history = model.fit(X_train_m, y_train_m, validation_data=(X_valid_m, y_valid_m), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs,\n",
    "                   callbacks=[earlyStopping, mcp_save, reduce_lr_loss])  # for regularization)\n",
    "#history = model.fit(X_train_m, y_train_m, batch_size=batch_size, epochs=num_epochs)\n",
    "#model.fit(X_trainn, y_trainn, batch_size=batch_size, epochs=num_epochs)\n",
    "#model.fit(X_train_m, y_train_m, batch_size=batch_size, epochs=num_epochs)\n",
    "#mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "plt.plot(history.history[\"accuracy\"], 'g--', label='train accuracy')\n",
    "plt.plot(history.history[\"loss\"], 'r--', label='train loss')\n",
    "plt.plot(history.history[\"val_loss\"], 'r:', label='validation loss')\n",
    "plt.plot(history.history[\"val_accuracy\"], 'g:', label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "######### ------------------------- ################### save the model\n",
    "#model.save('model/'+save_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef6d96",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82bcbd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.878104967948718"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('model/'+save_time+'-model.mdl_wts.hdf5')\n",
    "result = np.round(model.predict(X_valid_m))\n",
    "accuracy_score(y_valid_m, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53b244",
   "metadata": {},
   "source": [
    "## Sentiment results, Sampling some guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5583e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 2127\n",
      "#### negative --- ['Dear', 'Santa', 'Claus', 'I', 'Like', 'To', 'Nothing', 'But', 'The', 'Beat', 'David', 'Guetta', 'CD', 'For', 'Christmas']\n",
      "index 971\n",
      "#### negative --- ['The', 'original', 'line', 'blur', 'back', 'two', 'great', 'songs', '.', 'These', 'make', 'great', 'synthesis', 'classic', 'brit', 'pop', 'era', 'blur', 'became', 'mature', 'sound', 'eponymous', 'blur', '.', 'Under', 'Westway', 'ballad', 'Puritan', 'jaunty', 'number', 'finds', 'blur', 'looking', 'back', 'punk', 'new', 'wave', '.', 'For', 'buck', 'song', 'go', 'wrong', '.', 'Like', 'many', 'us', 'blur', 'fans', 'since', 'beginning', 'hope', 'portending', 'something', 'album', 'new', 'songs', 'perhaps', '.', 'A', 'limited', 'edition', 'CD', 'release', 'blur', 'website', 'containing', 'songs', 'plus', 'alternative', 'versions', '.']\n",
      "index 3309\n",
      "#### positive --- ['It', 'isn', 'full', 'song', '.', 'Plus', 'I', 'expecting', 'able', 'download', 'listen', 'via', 'Amazon', 'Music', 'app', 'I', 'use', '.']\n",
      "index 7547\n",
      "#### positive --- ['Another', 'underground', 'guy', 'sick', 'tunes', 'great', 'message', 'solid', 'Word', '.', 'None', 'songs', 'titles', 'really', 'appealed', 'looked', 'guys', 'repping', 'Christ', 'I', 'especially', 'hesitated', 'album', 'called', 'Mic', 'Check', 'm', 'glad', 'previewed', 'anyway', '!', 'Pretty', 'cool', '!', 'Keep', 'thing', 'saturating', 'albums', 'Truth', '!', '!', 'Love', '.']\n",
      "index 871\n",
      "#### positive --- ['Why', 'I', 'charged', 'full', 'price', 'song', 'CD', 'seconds', '?', '?', 'I', 'know', 'responsible', '.', '.', '.', 'artist', 'Amazon', '.', 'I', 'would', 'appreciate', 'explanation', 'Amazon', 'though', '.', 'Thank', '.']\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "acc_count = 0\n",
    "res_final = []\n",
    "for r in result:\n",
    "    acc_count+=1\n",
    "    if r==0:\n",
    "        res_final.append(\"negative\")\n",
    "    if r==1:\n",
    "        res_final.append(\"positive\")\n",
    "        \n",
    "for i in random.sample(range(0, 9993), 5):\n",
    "    print(\"index\", i)\n",
    "    print(\"####\", res_final[i], \"---\", test_sent[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c2ed8ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9208/2903530646.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m#print(key)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mdef_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mres_count\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 tempdata.append({\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "json_raw_data = []\n",
    "jsonfilename = 'dataset/classification/tt.gz'\n",
    "counter = 0\n",
    "list_of_skip_indexies = [90, 117, 439, 2138, 4112, 5364, 8210]\n",
    "res_count = 0\n",
    "for line in gzip.open('dataset/classification/t_masked.gz'):\n",
    "    review_data = json.loads(line)\n",
    "    tempdata = []\n",
    "    if counter in list_of_skip_indexies: # keep origine\n",
    "        print(\"keep original\")\n",
    "        print(counter)\n",
    "        print(review_data)\n",
    "        res_count -=1\n",
    "        for key in review_data:\n",
    "            #print(\"keep original\")\n",
    "            tempdata.append({\n",
    "            key: str(review_data[key])})\n",
    "    else:\n",
    "        def_dict = defaultdict(list)\n",
    "        def_dict = dict()\n",
    "        for key in review_data:\n",
    "            #print(key)\n",
    "            if key == \"sentiment\":\n",
    "                def_dict[key] = res_final[res_count]\n",
    "            else:\n",
    "                tempdata.append({\n",
    "                key: str(review_data[key])})\n",
    "                def_dict[key] = str(review_data[key])\n",
    "    counter += 1\n",
    "    res_count +=1\n",
    "    #print(counter)\n",
    "\n",
    "    #json_raw_data.append(def_dict)\n",
    "    json_raw_data.append(def_dict) # ok this is very good, itsa opening with json.load\n",
    "    \n",
    "outFile = open('music_reviews_test.json', 'w')\n",
    "for instance in json_raw_data:\n",
    "      outFile.write(json.dumps(instance) + '\\n')\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in gzip.open('group16.json.gz'):\n",
    "    review_data = json.loads(line)\n",
    "    for key in review_data:\n",
    "        print('\"' + key +'\": ' + str(review_data[key]))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
