{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWuqOkEX5jkD",
    "outputId": "eacc24b8-e1ea-4705-fb63-ca4e0254023d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting preprocessing\n",
      "  Downloading preprocessing-0.1.13-py3-none-any.whl (349 kB)\n",
      "\u001b[K     |████████████████████████████████| 349 kB 8.7 MB/s \n",
      "\u001b[?25hCollecting sphinx-rtd-theme==0.2.4\n",
      "  Downloading sphinx_rtd_theme-0.2.4-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 42.2 MB/s \n",
      "\u001b[?25hCollecting nltk==3.2.4\n",
      "  Downloading nltk-3.2.4.tar.gz (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 55.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.2.4->preprocessing) (1.15.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.2.4-py3-none-any.whl size=1367722 sha256=b34d471d2762336f42d18b6e469a1d5cfe53ca263686e96d5b32c9cefdb89857\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/5e/9e/4cb46185f2a16c60e6fc524372ba7fef89ce3347734c8798b6\n",
      "Successfully built nltk\n",
      "Installing collected packages: sphinx-rtd-theme, nltk, preprocessing\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.2.4 preprocessing-0.1.13 sphinx-rtd-theme-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcH_1liR48DC",
    "outputId": "3ab1b240-a63f-4b75-83dc-01ba68cc2aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '2ndYearProject-NLP'...\n",
      "remote: Enumerating objects: 544, done.\u001b[K\n",
      "remote: Counting objects: 100% (216/216), done.\u001b[K\n",
      "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
      "remote: Total 544 (delta 107), reused 163 (delta 56), pack-reused 328\u001b[K\n",
      "Receiving objects: 100% (544/544), 283.61 MiB | 18.91 MiB/s, done.\n",
      "Resolving deltas: 100% (272/272), done.\n",
      "Checking out files: 100% (114/114), done.\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 10.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 51.1 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 4.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 42.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[K     |████████████████████████████████| 287 kB 7.6 MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 72.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!git clone https://github.com/csipapicsa/2ndYearProject-NLP.git\n",
    "!pip install transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import urllib.request\n",
    "import csv\n",
    "import importlib \n",
    "!pip install contractions\n",
    "f = importlib.import_module('2ndYearProject-NLP.functions')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQX6kRkh5iOM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYawEtQs5EFb",
    "outputId": "2c27a32b-74d4-4872-cac0-da3e146c6541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  100000\n",
      "Number of data:  10000\n",
      "Number of data:  10000\n",
      "[4603, 4838, 16019, 18229, 19785, 23315, 28397, 28834, 33359, 43138, 43539, 43690, 44008, 44477, 44972, 48811, 49317, 50106, 51717, 52286, 55555, 56171, 57223, 58328, 58799, 58866, 59525, 59739, 61046, 61914, 61916, 62831, 63208, 72268, 78944, 79067, 80093, 80637, 80658, 81640, 81900, 82510, 83138, 83145, 83615, 84761, 87112, 88895, 88984, 89132, 91949, 94301, 94727, 99641]\n",
      "[2900, 4294, 5135, 8540]\n"
     ]
    }
   ],
   "source": [
    "PATH = {}\n",
    "PATH[\"dataset_classification\"] = \"2ndYearProject-NLP/dataset/classification/\"\n",
    "PATH[\"dataset_labeling\"] = \"2ndYearProject-NLP/dataset/seq_labeling/\"\n",
    "PATH[\"music_reviews_train\"] = PATH[\"dataset_classification\"] + \"music_reviews_train.json.gz\"\n",
    "PATH[\"music_reviews_dev\"] = PATH[\"dataset_classification\"] + \"music_reviews_dev.json.gz\"\n",
    "PATH[\"music_reviews_test\"] = PATH[\"dataset_classification\"] + \"music_reviews_test_masked.json.gz\"\n",
    "PATH[\"hard_sentences\"] = PATH[\"dataset_classification\"] + \"hard_sentences.json.gz\"\n",
    "\n",
    "train = f.readJson(PATH[\"music_reviews_train\"])\n",
    "dev = f.readJson(PATH[\"music_reviews_dev\"])\n",
    "test = f.readJson(PATH[\"music_reviews_test\"])\n",
    "\n",
    "sent_dict = {\"positive\": 1, \"negative\": 0, \"POSITIVE\": 1, \"NEGATIVE\":0}\n",
    "\n",
    "# read the train data\n",
    "data = train\n",
    "train_sent = []\n",
    "train_sentiment = []\n",
    "train_idx = []\n",
    "train_missing_indexies = []\n",
    "y_train = []\n",
    "length_of_sentencies_counter = []\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        train_sent.append(data[i][\"reviewText\"])\n",
    "        train_sentiment.append(data[i][\"sentiment\"])\n",
    "        train_idx.append(i)\n",
    "        y_train.append(sent_dict[data[i][\"sentiment\"]])\n",
    "        length_of_sentencies_counter.append(len(data[i][\"reviewText\"].split()))\n",
    "    except KeyError:\n",
    "        train_missing_indexies.append(i)\n",
    "        continue\n",
    "print(train_missing_indexies)\n",
    "\n",
    "# read the dev data \n",
    "data = dev\n",
    "dev_sent = []\n",
    "dev_sentiment = []\n",
    "dev_idx = []\n",
    "dev_missing_indexies = []\n",
    "dev_y_train = []\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        dev_sent.append(data[i][\"reviewText\"])\n",
    "        dev_sentiment.append(data[i][\"sentiment\"])\n",
    "        dev_idx.append(i)\n",
    "        dev_y_train.append(sent_dict[data[i][\"sentiment\"]]) \n",
    "    except KeyError:\n",
    "        dev_missing_indexies.append(i)\n",
    "        continue\n",
    "print(dev_missing_indexies)\n",
    "\n",
    "\n",
    "\n",
    "# make dev_sentiment into vector for checking accuracy laters... \n",
    "dev_classvec = np.array([sent_dict[s] for s in dev_sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IX-fzLHUCoqq",
    "outputId": "5b2dd648-e501-4a5a-e219-83ec92aa5221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting auto-sklearn\n",
      "  Downloading auto-sklearn-0.14.7.tar.gz (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 8.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (57.4.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (4.2.0)\n",
      "Collecting distro\n",
      "  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.21.6)\n",
      "Collecting scipy>=1.7.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.1 MB 1.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.1.0)\n",
      "Collecting scikit-learn<0.25.0,>=0.24.0\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3 MB 1.4 MB/s \n",
      "\u001b[?25hCollecting dask>=2021.12\n",
      "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 31.4 MB/s \n",
      "\u001b[?25hCollecting distributed>=2012.12\n",
      "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
      "\u001b[K     |████████████████████████████████| 837 kB 42.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (6.0)\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.3.5)\n",
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.1.0)\n",
      "Collecting ConfigSpace<0.5,>=0.4.21\n",
      "  Downloading ConfigSpace-0.4.21-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 27.1 MB/s \n",
      "\u001b[?25hCollecting pynisher<0.7,>=0.6.3\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
      "Collecting pyrfr<0.9,>=0.8.1\n",
      "  Downloading pyrfr-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 39.9 MB/s \n",
      "\u001b[?25hCollecting smac<1.3,>=1.2\n",
      "  Downloading smac-1.2.tar.gz (260 kB)\n",
      "\u001b[K     |████████████████████████████████| 260 kB 56.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (0.29.30)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (3.0.9)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.12->auto-sklearn) (1.3.0)\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 50.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.12->auto-sklearn) (21.3)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask>=2021.12->auto-sklearn) (0.11.2)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (7.1.2)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (2.2.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (1.0.3)\n",
      "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (5.1.1)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (1.7.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (2.11.3)\n",
      "Collecting cloudpickle>=1.1.1\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (5.4.8)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2012.12->auto-sklearn) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2022.1)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.15.0)\n",
      "Collecting emcee>=3.0.0\n",
      "  Downloading emcee-3.1.2-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 4.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2012.12->auto-sklearn) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2012.12->auto-sklearn) (2.0.1)\n",
      "Building wheels for collected packages: auto-sklearn, pynisher, smac, liac-arff\n",
      "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for auto-sklearn: filename=auto_sklearn-0.14.7-py3-none-any.whl size=6602873 sha256=0f40b04bbc6335c8610f12f70f74876a37af19c6f1c23737a02a848678cf6d74\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/43/5c/2fbe6fd19e3af314cbc4aa808378068d8ddd6792064f4a2448\n",
      "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7043 sha256=19c15b3a35dd3ecee67692545426474965ce313ab427c6c7c6e0300bb23e2cbc\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/71/95/7555ec3253e1ba8add72ae5febf1b015d297f3b73ba296d6f6\n",
      "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smac: filename=smac-1.2-py3-none-any.whl size=215930 sha256=ca52ced88e9350c36972446f10202964967e054ff84352909314eb62aac2bd34\n",
      "  Stored in directory: /root/.cache/pip/wheels/ad/95/67/6afc6b04d3715070c853d0a9d7c7b1fb822def38671dfbbb9f\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11732 sha256=67ecb7b7e8941226152144e180730c5f25b81dece654215260972001fa312204\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
      "Successfully built auto-sklearn pynisher smac liac-arff\n",
      "Installing collected packages: locket, partd, fsspec, cloudpickle, scipy, dask, scikit-learn, pyrfr, pynisher, emcee, distributed, ConfigSpace, smac, liac-arff, distro, auto-sklearn\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.3.0\n",
      "    Uninstalling cloudpickle-1.3.0:\n",
      "      Successfully uninstalled cloudpickle-1.3.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2.12.0\n",
      "    Uninstalling dask-2.12.0:\n",
      "      Successfully uninstalled dask-2.12.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: distributed\n",
      "    Found existing installation: distributed 1.25.3\n",
      "    Uninstalling distributed-1.25.3:\n",
      "      Successfully uninstalled distributed-1.25.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
      "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed ConfigSpace-0.4.21 auto-sklearn-0.14.7 cloudpickle-2.1.0 dask-2022.2.0 distributed-2022.2.0 distro-1.7.0 emcee-3.1.2 fsspec-2022.5.0 liac-arff-2.5.0 locket-1.0.0 partd-1.2.0 pynisher-0.6.4 pyrfr-0.8.2 scikit-learn-0.24.2 scipy-1.7.3 smac-1.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "cloudpickle",
         "scipy",
         "sklearn"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jKlx08Z35EIS"
   },
   "outputs": [],
   "source": [
    "#!pip install pandas \n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "3nfRjglW9J3A",
    "outputId": "3b4a45ef-a741-4734-a154-22c4f41f1f4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e269da88-fb39-49b3-a81f-8679e6d4f98e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So creative!  Love his music - the words, the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tape can hardly be understood and it was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buy the CD.  Do not buy the MP3 album.  Downlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love Dallas Holms music and voice!  Thank Yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great memories of my early years in Christ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have been listening to this album set my ENT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love all of his music!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Love Talbot music very inspiring and since thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Most unique  sound.  Nothing  even  similar  a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The item ordered arrived in a timely manner.  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e269da88-fb39-49b3-a81f-8679e6d4f98e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e269da88-fb39-49b3-a81f-8679e6d4f98e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e269da88-fb39-49b3-a81f-8679e6d4f98e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  So creative!  Love his music - the words, the ...          1\n",
       "1  This tape can hardly be understood and it was ...          0\n",
       "2  Buy the CD.  Do not buy the MP3 album.  Downlo...          0\n",
       "3  I love Dallas Holms music and voice!  Thank Yo...          1\n",
       "4         Great memories of my early years in Christ          1\n",
       "5  I have been listening to this album set my ENT...          1\n",
       "6                          I love all of his music!!          1\n",
       "7  Love Talbot music very inspiring and since thi...          1\n",
       "8  Most unique  sound.  Nothing  even  similar  a...          1\n",
       "9  The item ordered arrived in a timely manner.  ...          1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['Review'] = pd.DataFrame(train_sent)\n",
    "data['Sentiment'] = y_train\n",
    "\n",
    "dev = pd.DataFrame()\n",
    "dev['Review'] = pd.DataFrame(dev_sent)\n",
    "dev['Sentiment'] = dev_classvec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BteKlbr15ENz",
    "outputId": "c93e4905-3b81-4785-d17e-4aa4faf2df41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    60734\n",
       "0    39212\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.head(10)\n",
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "kB5mKQMgUHLw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5NhqGhiTDJEY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8RsS3u1CCjot"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9-CfLpO9Fkj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TmTEqTcc5EQh"
   },
   "outputs": [],
   "source": [
    "train = np.array(data['Review'])\n",
    "train_labels = np.array(data['Sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2D9j39FjBBK",
    "outputId": "8b05c8e6-918c-4982-bc41-87338c0bf262"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       "       'This tape can hardly be understood and it was listed for sale as \"very good\".  It\\'s VERY BAD.',\n",
       "       \"Buy the CD.  Do not buy the MP3 album.  Download is no longer available.  But you don't find that out until after you have purchased it.\",\n",
       "       ..., \"I'm enjoying her album very much!\",\n",
       "       'These digital files are corrupted.  There are glitches in the recording -- VERY NOTICEABLE GLITCHES.  Extremely disappointed as I\\'ve enjoyed other volumes of Knardahl\\'s recordings.  Specifically, the opening of \"Lyric Pieces, Book 10, Op. 71: IV. Skogstillhet (Peace of Woods)\".',\n",
       "       \"I saw the Tempest live performed by the Minnesota Orchestra and liked it so much I wanted a recording. This album was a big disappointment, it's too quiet and the sound seems to almost fade at times. I will probably delete it from my iPod.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDg8LTR85ETE",
    "outputId": "e8a8266c-f524-44b8-da02-ce2af4834181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_12 (KerasLayer)  (None, 50)               48190600  \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 160)               8160      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               16100     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,214,961\n",
      "Trainable params: 48,214,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "\n",
    "\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "hub_layer(train[:10])\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(160, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(100, activation = 'tanh'))\n",
    "model.add(tf.keras.layers.Dense(1)) ##outputs the log-odds of the true class\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "W_cgVPpL5EVd"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvykuqEG5EX9",
    "outputId": "13b5db0d-d6ea-40ed-f013-74e69cd89b9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       "       'This tape can hardly be understood and it was listed for sale as \"very good\".  It\\'s VERY BAD.',\n",
       "       \"Buy the CD.  Do not buy the MP3 album.  Download is no longer available.  But you don't find that out until after you have purchased it.\",\n",
       "       ...,\n",
       "       \"An 11th house classic...I, too own the original vinyl since the 70's...hot playing and some great feelings....better get a turntable as I wouldn't hold my breath for this to come out on cd!\",\n",
       "       'By far one of the best songs over the last 30 years. OH YES',\n",
       "       \"Another song that I've been wanting\"], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_val = train[:2000]\n",
    "subset_x_train = train[2000:]\n",
    "\n",
    "y_val = train_labels[:2000]\n",
    "subset_y_train = train_labels[2000:]\n",
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zpA8Dk55EaK",
    "outputId": "dec35fb9-9507-4537-cc96-70f3d5092250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1934/4898 [==========>...................] - ETA: 26:15 - loss: 0.3244 - accuracy: 0.8615"
     ]
    }
   ],
   "source": [
    "history = model.fit(subset_x_train, \n",
    "                    subset_y_train, \n",
    "                    epochs = 5,\n",
    "                    batch_size = 20,\n",
    "                    validation_data = (x_val, y_val), \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoBGZJJe5Eck"
   },
   "outputs": [],
   "source": [
    "def init_log_for_training():\n",
    "    dateTimeObj = datetime.now()\n",
    "    save_time = str(dateTimeObj.year)+'-'+str(dateTimeObj.month)+'-'+str(dateTimeObj.day)+'-'+str(dateTimeObj.hour)+'-'+str(dateTimeObj.minute)+'-'+str(dateTimeObj.second)\n",
    "    columns = [\"Running ID\", \"Model Name\", \"Expand Contractions\", \"Basic Preprocessing\", \n",
    "           \"Grammar Correction\", \"Simplify Negotiations\", \"Lemmatize\", \"Remove Stop Words\", \"No. of Sentences\", \n",
    "           \"Train Accuracy STOP\", \"Test Accuracy STOP\", \"Train Loss STOP\", \"Test Loss STOP\",\n",
    "           \"Train_sentence_fully_catched_ratio\", \"Test_sentence_fully_catched_ratio\",\n",
    "           \"Length of Sentence\", \"Batch size of RNN\"]\n",
    "    # dataframe\n",
    "    results_dataframe = pd.DataFrame(columns=columns)\n",
    "    return results_dataframe, save_time"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
